import gspread
from google.oauth2.service_account import Credentials
import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin
from langdetect import detect
from deep_translator import GoogleTranslator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Google Sheets
_client = None

def authenticate_google_sheets():
    global _client
    if _client is not None:
        return _client

    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    try:
        creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
        _client = gspread.authorize(creds)
        logging.info("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets.")
        return _client
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        raise

def get_business_websites(sheet_name):
    try:
        client = authenticate_google_sheets()
        sheet = client.open(sheet_name).sheet1
        websites = sheet.col_values(1)
        num_sites = len(websites) - 1
        logging.info(f"üìå –ü–æ–ª—É—á–µ–Ω–æ {num_sites} —Å–∞–π—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.")
        return websites[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        return []

def detect_page_language(html):
    try:
        soup = BeautifulSoup(html, "html.parser")
        lang_tag = soup.find("html").get("lang")
        if lang_tag:
            return lang_tag.split("-")[0]
        return detect(html)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return "en"

def translate_keywords(keywords, target_lang):
    try:
        translated_keywords = [GoogleTranslator(source="en", target=target_lang).translate(word) for word in keywords]
        logging.info(f"üåç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –Ω–∞ {target_lang}: {translated_keywords}")
        return translated_keywords
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return keywords

def find_job_pages(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        page_lang = detect_page_language(response.text)
        keywords = ["job", "career", "careers", "jobs", "hiring", "employment", "join us", "work with us", "vacancies", "karriere"]
        
        job_links = set()
        for link in soup.find_all("a", href=True):
            href = link["href"].lower()
            if any(keyword in href for keyword in keywords):
                full_url = urljoin(url, link["href"])
                job_links.add(full_url)
        
        if not job_links and page_lang != "en":
            logging.info(f"üåç –ü–µ—Ä–µ–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ {page_lang} –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è {url}.")
            translated_keywords = translate_keywords(keywords, page_lang)
            for link in soup.find_all("a", href=True):
                href = link["href"].lower()
                if any(keyword in href for keyword in translated_keywords):
                    full_url = urljoin(url, link["href"])
                    job_links.add(full_url)
        
        if job_links:
            logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(job_links)} —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–∞ {url}.")
        else:
            logging.warning(f"‚ö†Ô∏è –ù–∞ {url} —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        
        return list(job_links) if job_links else None
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ {url}: {e}")
        return None

def write_job_urls_to_sheet(sheet_name, websites, job_urls):
    try:
        client = authenticate_google_sheets()
        sheet = client.open(sheet_name).sheet1
        headers = sheet.row_values(1)
        
        if "Job Url" not in headers:
            sheet.update_cell(1, 2, "Job Url")
        
        for i, urls in enumerate(job_urls, start=2):
            if urls:
                sheet.update_cell(i, 2, ", ".join(urls))
            else:
                sheet.update_cell(i, 2, "–Ω–µ—Ç URL")
        
        logging.info("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ Google Sheets.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É: {e}")

def main():
    sheet_name = "Parser"
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    websites = get_business_websites(sheet_name)
    
    if not websites:
        logging.warning("‚ö†Ô∏è –ù–µ—Ç —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
        return
    
    logging.info("üîé –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏...")
    job_urls = [find_job_pages(site) for site in websites]
    
    logging.info("‚úçÔ∏è –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü—É...")
    write_job_urls_to_sheet(sheet_name, websites, job_urls)
    
    logging.info("üéØ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()
