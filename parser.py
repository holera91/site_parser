# —à–ø—Ä–∞—Ü—é—î –∑ 1 –ª–∏—Å—Ç–æ–º —Ñ–∞–π–ª—É Parser, —à—É–∫–∞—î —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –≤–∞–∫–∞–Ω—Å—ñ—è–º–∏, 
# –ø–æ—Ç—ñ–º –ø—Ä–æ—Ö–æ–¥–∏—Ç—å—Å—è —ñ –Ω–∞–º–∞–≥–∞—î—Ç—å—Å—è –∑–Ω–∞–π—Ç–∏ –Ω–∞ –Ω–∏—Ö –≤–∞–∫–∞–Ω—Å—ñ—ó
import gspread
from google.oauth2.service_account import Credentials
import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin
from langdetect import detect
from deep_translator import GoogleTranslator
import re
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Google Sheets
_client = None

def authenticate_google_sheets():
    global _client
    if (_client is not None):
        return _client

    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    try:
        creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
        _client = gspread.authorize(creds)
        logging.info("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets.")
        return _client
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        raise

def get_business_websites(sheet_name):
    try:
        client = authenticate_google_sheets()
        sheet = client.open(sheet_name).sheet1
        websites = sheet.col_values(1)
        num_sites = len(websites) - 1
        logging.info(f"üìå –ü–æ–ª—É—á–µ–Ω–æ {num_sites} —Å–∞–π—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Å—ã–ª–æ–∫
        corrected_websites = []
        for website in websites[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            website = website.strip()
            if not website.startswith("http://") and not website.startswith("https://"):
                website = "https://" + website
            if not website.endswith("/"):
                website += "/"
            corrected_websites.append(website)
        
        return corrected_websites
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        return []

def detect_page_language(html):
    try:
        soup = BeautifulSoup(html, "html.parser")
        lang_tag = soup.find("html").get("lang")
        if lang_tag:
            return lang_tag.split("-")[0]
        return detect(html)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return "en"

def translate_keywords(keywords, target_lang):
    try:
        translated_keywords = [GoogleTranslator(source="en", target=target_lang).translate(word) for word in keywords]
        logging.info(f"üåç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –Ω–∞ {target_lang}: {translated_keywords}")
        return translated_keywords
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return keywords

def find_job_pages(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        page_lang = detect_page_language(response.text)
        keywords = ["job", "career", "careers", "jobs", "hiring", "employment", "join us", "work with us", "vacancies", "karriere" , "working at", "vacancy", "job openings"]
        
        job_links = set()
        base_links = set()  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
        
        for link in soup.find_all("a", href=True):
            href = link["href"].lower()
            if href.startswith("mailto:"):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å mailto:
            if any(keyword in href for keyword in keywords):
                full_url = urljoin(url, link["href"])
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –±–∞–∑–æ–≤–æ–π –∏–ª–∏ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–æ–º
                is_subpath = False
                for base_link in base_links:
                    if full_url.startswith(base_link + "/"):
                        is_subpath = True
                        break
                
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
                if not is_subpath:
                    job_links.add(full_url)
                    # –ï—Å–ª–∏ —ç—Ç–æ –±–∞–∑–æ–≤–∞—è —Å—Å—ã–ª–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –≤ base_links
                    if not any(c in full_url for c in ["/", "?", "#"]):  # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∞–∑–æ–≤—É—é —Å—Å—ã–ª–∫—É
                        base_links.add(full_url)
        
        if not job_links and page_lang != "en":
            logging.info(f"üåç –ü–µ—Ä–µ–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ {page_lang} –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è {url}.")
            translated_keywords = translate_keywords(keywords, page_lang)
            for link in soup.find_all("a", href=True):
                href = link["href"].lower()
                if href.startswith("mailto:"):
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å mailto:
                if any(keyword in href for keyword in translated_keywords):
                    full_url = urljoin(url, link["href"])
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–æ–º
                    is_subpath = False
                    for base_link in base_links:
                        if full_url.startswith(base_link + "/"):
                            is_subpath = True
                            break
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
                    if not is_subpath:
                        job_links.add(full_url)
                        # –ï—Å–ª–∏ —ç—Ç–æ –±–∞–∑–æ–≤–∞—è —Å—Å—ã–ª–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –≤ base_links
                        if not any(c in full_url for c in ["/", "?", "#"]):
                            base_links.add(full_url)
        
        if job_links:
            logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(job_links)} —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–∞ {url}.")
        else:
            logging.warning(f"‚ö†Ô∏è –ù–∞ {url} —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        
        return list(job_links) if job_links else None
    except requests.exceptions.SSLError:
        logging.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–π—Ç {url} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ SSL.")
        return None
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ {url}: {e}")
        return None

def find_emails(html):
    """
    –ò—â–µ—Ç email-–∞–¥—Ä–µ—Å–∞ –≤ HTML-–∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    try:
        email_patterns = [
            r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            r"[a-zA-Z0-9._%+-]+\s@\s[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            r"[a-zA-Z0-9._%+-]+\s\(at\)\s[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            r"[a-zA-Z0-9._%+-]+\s*\(at\)\s*[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
        ]
        emails = set()
        for pattern in email_patterns:
            matches = re.findall(pattern, html)
            for match in matches:
                cleaned_email = match.replace(" ", "").replace("(at)", "@")
                emails.add(cleaned_email)
        logging.info(f"üìß –ù–∞–π–¥–µ–Ω–æ {len(emails)} email-–∞–¥—Ä–µ—Å–æ–≤.")
        return list(emails)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ email-–∞–¥—Ä–µ—Å–æ–≤: {e}")
        return []

def write_job_urls_and_emails_to_sheet(sheet, row, job_urls, emails):
    try:
        if job_urls:
            sheet.update_cell(row, 2, ", ".join(job_urls))
        else:
            sheet.update_cell(row, 2, "–Ω–µ—Ç URL")
        
        existing_emails = sheet.cell(row, 3).value
        if existing_emails:
            emails = list(set(existing_emails.split(", ") + emails))
        if emails:
            sheet.update_cell(row, 3, ", ".join(emails))
        else:
            sheet.update_cell(row, 3, "–Ω–µ—Ç email")
        
        logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {row} —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ Google Sheets.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {row}: {e}")

def parse_job_page(url):
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –∏ –∏—â–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_lang = detect_page_language(response.text)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = [
            "Software Developer", "Data Engineer", "Software Architect",
            "Designer", "Data Scientist", "IT Manager", "DevOps"
        ]
        
        # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if page_lang != "en":
            translated_keywords = translate_keywords(keywords, page_lang)
            keywords.extend(translated_keywords)
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        found_positions = set()
        for keyword in keywords:
            if keyword.lower() in soup.get_text().lower():
                if detect(keyword) != "en":  # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –ø–µ—Ä–µ–≤–æ–¥–∏–º
                    translated = GoogleTranslator(source=page_lang, target="en").translate(keyword)
                    found_positions.add(translated)
                else:
                    found_positions.add(keyword)
        
        if found_positions:
            return ", ".join(found_positions)
        else:
            return "No relevant positions found"
    except requests.exceptions.SSLError:
        logging.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–π—Ç {url} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ SSL.")
        return "SSL Error"
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return "Error parsing page"

def update_open_positions(sheet, row, job_urls):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É Open Position –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.
    """
    try:
        results = []
        for url in job_urls:
            if url:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                result = parse_job_page(url)
                results.append(result)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–æ–ª–æ–Ω–∫—É Open Position
        if results:
            sheet.update_cell(row, 4, ", ".join(results))
        else:
            sheet.update_cell(row, 4, "No relevant positions found")
        
        logging.info(f"‚úÖ –ö–æ–ª–æ–Ω–∫–∞ Open Position –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {row} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–æ–ª–æ–Ω–∫–∏ Open Position –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {row}: {e}")

def main():
    sheet_name = "Parser"
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    websites = get_business_websites(sheet_name)
    
    if not websites:
        logging.warning("‚ö†Ô∏è –ù–µ—Ç —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
        return
    
    logging.info("üîé –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –∏ email-–∞–¥—Ä–µ—Å–æ–≤...")
    
    client = authenticate_google_sheets()
    sheet = client.open(sheet_name).sheet1
    
    for row, website in enumerate(websites, start=2):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        logging.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–∞: {website}")
        
        # –ü–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
        job_urls = find_job_pages(website)
        
        # –ü–æ–∏—Å–∫ email-–∞–¥—Ä–µ—Å–æ–≤
        response = requests.get(website)
        emails = find_emails(response.text)
        
        # –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Google Sheets
        write_job_urls_and_emails_to_sheet(sheet, row, job_urls, emails)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ Open Position
        if job_urls:
            update_open_positions(sheet, row, job_urls)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(1)
    
    logging.info("üéØ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()