import gspread
from google.oauth2.service_account import Credentials
import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin
from langdetect import detect
from deep_translator import GoogleTranslator

def translate_keywords(keywords, target_lang):
    try:
        translated = [GoogleTranslator(source='auto', target=target_lang).translate(keyword) for keyword in keywords]
        return translated
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return keywords
from concurrent.futures import ThreadPoolExecutor

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def detect_page_language(text):
    try:
        return detect(text)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return "unknown"

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Google Sheets
_client = None

# HTTP-–∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Referer": "https://www.google.com/"
}

def authenticate_google_sheets():
    global _client
    if _client is not None:
        return _client

    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    try:
        creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
        _client = gspread.authorize(creds)
        logging.info("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets.")
        return _client
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        raise

def find_job_pages(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        page_lang = detect_page_language(response.text)
        keywords = ["job", "career", "careers", "jobs", "hiring", "employment", "join us", "work with us", "vacancies", "karriere"]
        
        job_links = set()
        base_links = set()
        
        for link in soup.find_all("a", href=True):
            href = link["href"].lower()
            if any(keyword in href for keyword in keywords):
                full_url = urljoin(url, link["href"])
                
                is_subpath = False
                for base_link in base_links:
                    if full_url.startswith(base_link + "/"):
                        is_subpath = True
                        break
                
                if not is_subpath:
                    job_links.add(full_url)
                    if not any(c in full_url for c in ["/", "?", "#"]):
                        base_links.add(full_url)
        
        if not job_links and page_lang != "en":
            logging.info(f"üåç –ü–µ—Ä–µ–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ {page_lang} –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è {url}.")
            translated_keywords = translate_keywords(keywords, page_lang)
            for link in soup.find_all("a", href=True):
                href = link["href"].lower()
                if any(keyword in href for keyword in translated_keywords):
                    full_url = urljoin(url, link["href"])
                    
                    is_subpath = False
                    for base_link in base_links:
                        if full_url.startswith(base_link + "/"):
                            is_subpath = True
                            break
                    
                    if not is_subpath:
                        job_links.add(full_url)
                        if not any(c in full_url for c in ["/", "?", "#"]):
                            base_links.add(full_url)
        
        if job_links:
            logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(job_links)} —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–∞ {url}.")
        else:
            logging.warning(f"‚ö†Ô∏è –ù–∞ {url} —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        
        return list(job_links) if job_links else None
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ {url}: {e}")
        return None

def parse_job_page(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        page_lang = detect_page_language(response.text)
        keywords = [
            "Software Developer", "Data Engineer", "Software Architect",
            "Designer", "Data Scientist", "IT Manager", "DevOps"
        ]
        
        if page_lang != "en":
            translated_keywords = translate_keywords(keywords, page_lang)
            keywords.extend(translated_keywords)
        
        found_positions = set()
        for keyword in keywords:
            if keyword.lower() in soup.get_text().lower():
                found_positions.add(keyword)
        
        return ", ".join(found_positions) if found_positions else "No relevant positions found"
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return "Error parsing page"

def main():
    sheet_name = "Parser"
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    websites = ["https://www.codigi.com/"]  # –ó–∞–≥–ª—É—à–∫–∞, –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–∞–π—Ç–æ–≤ –∏–∑ Google Sheets
    
    logging.info("üîé –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        job_urls = list(executor.map(find_job_pages, websites))
    
    logging.info("üîç –ü–∞—Ä—Å–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...")
    for urls in job_urls:
        if urls:
            for job_url in urls:
                parse_job_page(job_url)
    
    logging.info("üéØ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()
