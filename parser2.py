import gspread
from google.oauth2.service_account import Credentials
import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin
from langdetect import detect
from deep_translator import GoogleTranslator
from concurrent.futures import ThreadPoolExecutor
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Google Sheets
_client = None

def authenticate_google_sheets():
    global _client
    if (_client is not None):
        return _client

    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    try:
        creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
        _client = gspread.authorize(creds)
        logging.info("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Sheets.")
        return _client
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        raise

def get_business_websites(sheet_name):
    try:
        client = authenticate_google_sheets()
        sheet = client.open(sheet_name).sheet1
        websites = sheet.col_values(1)
        num_sites = len(websites) - 1
        logging.info(f"üìå –ü–æ–ª—É—á–µ–Ω–æ {num_sites} —Å–∞–π—Ç–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã.")
        return websites[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        return []

def detect_page_language(html):
    try:
        soup = BeautifulSoup(html, "html.parser")
        lang_tag = soup.find("html").get("lang")
        if lang_tag:
            return lang_tag.split("-")[0]
        return detect(html)
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return "en"

def translate_keywords(keywords, target_lang):
    try:
        translated_keywords = [GoogleTranslator(source="en", target=target_lang).translate(word) for word in keywords]
        logging.info(f"üåç –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –Ω–∞ {target_lang}: {translated_keywords}")
        return translated_keywords
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return keywords

def find_job_pages(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        page_lang = detect_page_language(response.text)
        keywords = ["job", "career", "careers", "jobs", "hiring", "employment", "join us", "work with us", "vacancies", "karriere"]
        
        job_links = set()
        base_links = set()  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
        
        for link in soup.find_all("a", href=True):
            href = link["href"].lower()
            if href.startswith("mailto:"):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å mailto:
            if any(keyword in href for keyword in keywords):
                full_url = urljoin(url, link["href"])
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –±–∞–∑–æ–≤–æ–π –∏–ª–∏ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–æ–º
                is_subpath = False
                for base_link in base_links:
                    if full_url.startswith(base_link + "/"):
                        is_subpath = True
                        break
                
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
                if not is_subpath:
                    job_links.add(full_url)
                    # –ï—Å–ª–∏ —ç—Ç–æ –±–∞–∑–æ–≤–∞—è —Å—Å—ã–ª–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –≤ base_links
                    if not any(c in full_url for c in ["/", "?", "#"]):  # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∞–∑–æ–≤—É—é —Å—Å—ã–ª–∫—É
                        base_links.add(full_url)
        
        if not job_links and page_lang != "en":
            logging.info(f"üåç –ü–µ—Ä–µ–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ {page_lang} –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–∏—Å–∫ –¥–ª—è {url}.")
            translated_keywords = translate_keywords(keywords, page_lang)
            for link in soup.find_all("a", href=True):
                href = link["href"].lower()
                if href.startswith("mailto:"):
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å mailto:
                if any(keyword in href for keyword in translated_keywords):
                    full_url = urljoin(url, link["href"])
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–æ–º
                    is_subpath = False
                    for base_link in base_links:
                        if full_url.startswith(base_link + "/"):
                            is_subpath = True
                            break
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
                    if not is_subpath:
                        job_links.add(full_url)
                        # –ï—Å–ª–∏ —ç—Ç–æ –±–∞–∑–æ–≤–∞—è —Å—Å—ã–ª–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –≤ base_links
                        if not any(c in full_url for c in ["/", "?", "#"]):
                            base_links.add(full_url)
        
        if job_links:
            logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(job_links)} —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–∞ {url}.")
        else:
            logging.warning(f"‚ö†Ô∏è –ù–∞ {url} —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        
        return list(job_links) if job_links else None
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ {url}: {e}")
        return None

def find_emails(html):
    """
    –ò—â–µ—Ç email-–∞–¥—Ä–µ—Å–∞ –≤ HTML-–∫–æ–¥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    try:
        email_patterns = [
            r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            r"[a-zA-Z0-9._%+-]+\s@\s[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            r"[a-zA-Z0-9._%+-]+\s\(at\)\s[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            r"[a-zA-Z0-9._%+-]+\s*\(at\)\s*[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
        ]
        emails = set()
        for pattern in email_patterns:
            matches = re.findall(pattern, html)
            for match in matches:
                cleaned_email = match.replace(" ", "").replace("(at)", "@")
                emails.add(cleaned_email)
        logging.info(f"üìß –ù–∞–π–¥–µ–Ω–æ {len(emails)} email-–∞–¥—Ä–µ—Å–æ–≤.")
        return list(emails)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ email-–∞–¥—Ä–µ—Å–æ–≤: {e}")
        return []

def write_job_urls_and_emails_to_sheet(sheet_name, websites, job_urls, emails):
    try:
        client = authenticate_google_sheets()
        sheet = client.open(sheet_name).sheet1
        
        for i, (urls, email_list) in enumerate(zip(job_urls, emails), start=2):
            if urls:
                sheet.update_cell(i, 2, ", ".join(urls))
            else:
                sheet.update_cell(i, 2, "–Ω–µ—Ç URL")
            
            existing_emails = sheet.cell(i, 3).value
            if existing_emails:
                email_list = list(set(existing_emails.split(", ") + email_list))
            if email_list:
                sheet.update_cell(i, 3, ", ".join(email_list))
            else:
                sheet.update_cell(i, 3, "–Ω–µ—Ç email")
        
        logging.info("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ Google Sheets.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É: {e}")

def parse_job_page(url):
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –∏ –∏—â–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_lang = detect_page_language(response.text)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords = [
            "Software Developer", "Data Engineer", "Software Architect",
            "Designer", "Data Scientist", "IT Manager", "DevOps"
        ]
        
        # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if page_lang != "en":
            translated_keywords = translate_keywords(keywords, page_lang)
            keywords.extend(translated_keywords)
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        found_positions = set()
        for keyword in keywords:
            if keyword.lower() in soup.get_text().lower():
                if detect(keyword) != "en":  # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –ø–µ—Ä–µ–≤–æ–¥–∏–º
                    translated = GoogleTranslator(source=page_lang, target="en").translate(keyword)
                    found_positions.add(translated)
                else:
                    found_positions.add(keyword)
        
        if found_positions:
            return ", ".join(found_positions)
        else:
            return "No relevant positions found"
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return "Error parsing page"

def update_open_positions(sheet_name):
    """
    –ß–∏—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ Job Url –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É Open Position.
    """
    try:
        client = authenticate_google_sheets()
        sheet = client.open(sheet_name).sheet1
        
        # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ Job Url
        job_urls = sheet.col_values(2)
        for i, cell_value in enumerate(job_urls[1:], start=2):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if cell_value.strip().lower() == "–Ω–µ—Ç url":
                sheet.update_cell(i, 4, "check manually")
                continue
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏, –ø–∞—Ä—Å–∏–º –∏—Ö
            urls = [url.strip() for url in cell_value.split(",")]
            results = []
            for url in urls:
                if url:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    result = parse_job_page(url)
                    results.append(result)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–æ–ª–æ–Ω–∫—É Open Position
            if results:
                sheet.update_cell(i, 4, ", ".join(results))
            else:
                sheet.update_cell(i, 4, "No relevant positions found")
        
        logging.info("‚úÖ –ö–æ–ª–æ–Ω–∫–∞ Open Position —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–æ–ª–æ–Ω–∫–∏ Open Position: {e}")

def main():
    sheet_name = "Parser"
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    websites = get_business_websites(sheet_name)
    
    if not websites:
        logging.warning("‚ö†Ô∏è –ù–µ—Ç —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.")
        return
    
    logging.info("üîé –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ –∏ email-–∞–¥—Ä–µ—Å–æ–≤...")
    
    job_urls = []
    emails = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        for website in websites:
            job_urls.append(executor.submit(find_job_pages, website).result())
            response = requests.get(website)
            emails.append(find_emails(response.text))
    
    logging.info("‚úçÔ∏è –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—Ü—É...")
    write_job_urls_and_emails_to_sheet(sheet_name, websites, job_urls, emails)
    
    logging.info("üîç –ü–∞—Ä—Å–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫...")
    update_open_positions(sheet_name)
    
    logging.info("üéØ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()