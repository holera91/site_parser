import gspread
from google.oauth2.service_account import Credentials
import logging
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urlparse, urljoin
from langdetect import detect, DetectorFactory

# –£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏
DetectorFactory.seed = 0

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

_client = None

def authenticate_google_sheets():
    """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –≤ Google Sheets"""
    global _client
    if _client is not None:
        return _client

    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    try:
        creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
        _client = gspread.authorize(creds)
        logging.info("‚úÖ –£—Å–ø—ñ—à–Ω–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –≤ Google Sheets.")
        return _client
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó: {e}")
        raise

def get_sheet_data(spreadsheet_name):
    """–û—Ç—Ä–∏–º—É—î –¥–∞–Ω—ñ –∑ –ª–∏—Å—Ç–∞ 'SiteInfo' (Website, Site Language, –¢–∏–ø —Å–∞–π—Ç—É)"""
    try:
        client = authenticate_google_sheets()
        sheet = client.open(spreadsheet_name).worksheet("SiteInfo")
        data = sheet.get_all_records()

        websites = []
        site_language_col = None
        site_type_col = None

        for i, row in enumerate(data):
            if "Website" in row and row["Website"].strip():
                formatted_url = format_url(row["Website"])
                websites.append((i + 2, formatted_url))  # +2 —á–µ—Ä–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∏

            if site_language_col is None and "Site Language" in row:
                site_language_col = list(row.keys()).index("Site Language") + 1

            if site_type_col is None and "–¢–∏–ø —Å–∞–π—Ç—É" in row:
                site_type_col = list(row.keys()).index("–¢–∏–ø —Å–∞–π—Ç—É") + 1

        logging.info(f"üì• –ó–Ω–∞–π–¥–µ–Ω–æ {len(websites)} —Å–∞–π—Ç—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏.")
        return sheet, websites, site_language_col, site_type_col
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ Google Sheets: {e}")
        return None, [], None, None

def format_url(url):
    """–§–æ—Ä–º–∞—Ç—É—î URL —É –≤–∏–≥–ª—è–¥—ñ https://domain.com"""
    parsed = urlparse(url)
    if parsed.scheme in ["http", "https"]:
        return f"{parsed.scheme}://{parsed.netloc}"
    return f"https://{parsed.path.split('/')[0]}"

def detect_language(text):
    """–í–∏–∑–Ω–∞—á–∞—î –º–æ–≤—É —Ç–µ–∫—Å—Ç—É"""
    try:
        return detect(text)
    except Exception:
        return "unknown"

def get_site_language(url):
    """–í–∏–∑–Ω–∞—á–∞—î –º–æ–≤—É —Å–∞–π—Ç—É (meta description –∞–±–æ <p>)"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return "unknown"

        soup = BeautifulSoup(response.text, "html.parser")

        # –ü—Ä–æ–±—É—î–º–æ –≤–∑—è—Ç–∏ –º–æ–≤—É –∑ –º–µ—Ç–∞—Ç–µ–≥—É description
        meta_desc = soup.find("meta", attrs={"name": "description"})
        text = meta_desc["content"] if meta_desc else ""

        # –Ø–∫—â–æ –º–µ—Ç–∞-—Ç–µ–≥ –Ω–µ –¥–∞–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É, –±–µ—Ä–µ–º–æ –ø–µ—Ä—à–∏–π <p>
        if not text:
            paragraph = soup.find("p")
            text = paragraph.get_text(strip=True) if paragraph else ""

        return detect_language(text) if text else "unknown"
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤–∏ {url}: {e}")
        return "unknown"

def get_site_type(url):
    """–í–∏–∑–Ω–∞—á–∞—î —Ç–∏–ø —Å–∞–π—Ç—É: '–≤—ñ–∑–∏—Ç–∫–∞' –∞–±–æ '–±–∞–≥–∞—Ç–æ —Å—Ç–æ—Ä—ñ–Ω–æ–∫'"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return "unknown"

        soup = BeautifulSoup(response.text, "html.parser")
        domain = urlparse(url).netloc
        internal_links = set()

        for link in soup.find_all("a", href=True):
            href = link["href"]
            full_url = urljoin(url, href)
            if domain in urlparse(full_url).netloc and full_url != url:
                internal_links.add(full_url)

        return "–±–∞–≥–∞—Ç–æ —Å—Ç–æ—Ä—ñ–Ω–æ–∫" if len(internal_links) > 5 else "–≤—ñ–∑–∏—Ç–∫–∞"
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É —Å–∞–π—Ç—É {url}: {e}")
        return "unknown"

def get_next_empty_column(sheet):
    """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–∞—Å—Ç—É–ø–Ω—É –≤—ñ–ª—å–Ω—É –∫–æ–ª–æ–Ω–∫—É –ø—ñ—Å–ª—è 'Website'"""
    try:
        headers = sheet.row_values(1)
        if "Website" in headers:
            website_index = headers.index("Website")
            return website_index + 2  # –ù–∞—Å—Ç—É–ø–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –ø—ñ—Å–ª—è Website
        return None
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –≤—ñ–ª—å–Ω–æ—ó –∫–æ–ª–æ–Ω–∫–∏: {e}")
        return None

def scrape_about_page(url):
    """–ü–∞—Ä—Å–∏—Ç—å —Å–∞–π—Ç —ñ —à—É–∫–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫–æ–º–ø–∞–Ω—ñ—é"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return "‚ùå –°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"

        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        about_text = ""

        for p in paragraphs:
            text = p.get_text(strip=True)
            if 50 < len(text) < 500:  # –í—ñ–¥—Å—ñ—é—î–º–æ –∫–æ—Ä–æ—Ç–∫—ñ —Ç–∞ –¥–æ–≤–≥—ñ —Ç–µ–∫—Å—Ç–∏
                about_text = text
                break

        return about_text if about_text else "–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {url}: {e}")
        return "‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É"

def update_sheet(sheet, row, col, value):
    """–û–Ω–æ–≤–ª—é—î –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É —è—á–µ–π–∫—É –≤ Google Sheets"""
    try:
        sheet.update_cell(row, col, value)
        logging.info(f"‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ –≤ {row}:{col}")
    except Exception as e:
        logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ Google Sheets: {e}")

def main(spreadsheet_name):
    sheet, websites, site_language_col, site_type_col = get_sheet_data(spreadsheet_name)
    if not sheet or not websites or not site_language_col or not site_type_col:
        logging.error("‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö.")
        return

    next_column = get_next_empty_column(sheet)
    if not next_column:
        logging.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω—É –∫–æ–ª–æ–Ω–∫—É –¥–ª—è –¥–∞–Ω–∏—Ö.")
        return

    for row, website in websites:
        update_sheet(sheet, row, 2, website)

        site_language = get_site_language(website)
        update_sheet(sheet, row, site_language_col, site_language)

        site_type = get_site_type(website)
        update_sheet(sheet, row, site_type_col, site_type)

        about_info = scrape_about_page(website)
        update_sheet(sheet, row, next_column, about_info)

        time.sleep(2)  # –ê–Ω—Ç–∏–±–∞–Ω

    logging.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

# –ó–∞–ø—É—Å–∫ –∫–æ–¥—É
spreadsheet_name = "Parser"
main(spreadsheet_name)
